{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping links from main links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'py311 (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n py311 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode (no GUI)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Read URLs from the txt file\n",
    "with open('mainlinks.txt', 'r') as url_file:\n",
    "    urls = [line.strip() for line in url_file.readlines()]\n",
    "\n",
    "# Open file to write hrefs\n",
    "with open('srapedlink.txt', 'w') as file:\n",
    "    # Loop through each URL and extract hrefs\n",
    "    for i, url in enumerate(urls):\n",
    "        # Open the target URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load (adjust time if needed)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            # Locate the table inside the div with id 'content'\n",
    "            table = driver.find_element(By.XPATH, \"//div[@id='content']//table\")\n",
    "            \n",
    "            # Extract all anchor tags within the table\n",
    "            links = table.find_elements(By.TAG_NAME, \"a\")\n",
    "            \n",
    "            # Extract href attributes\n",
    "            hrefs = [link.get_attribute(\"href\") for link in links]\n",
    "            \n",
    "            # Write hrefs to a file, using a unique file for each URL\n",
    "            for href in hrefs:\n",
    "                file.write(href + '\\n')\n",
    "        \n",
    "            print(f\"Extracted {len(hrefs)} links from {url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from {url}: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping links from Sub links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode (no GUI)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Read URLs from the txt file\n",
    "with open('scrapedlink.txt', 'r') as url_file:\n",
    "    urls = [line.strip() for line in url_file.readlines()]\n",
    "\n",
    "# Open file to write hrefs\n",
    "with open('scrapedsublink.txt', 'w') as file:\n",
    "    # Loop through each URL and extract hrefs\n",
    "    for i, url in enumerate(urls):\n",
    "        # Open the target URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load (adjust time if needed)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            # Locate the table inside the div with id 'content'\n",
    "            table = driver.find_element(By.XPATH, \"//div[@id='content']//table\")\n",
    "            \n",
    "            # Extract all anchor tags within the table\n",
    "            links = table.find_elements(By.TAG_NAME, \"a\")\n",
    "            \n",
    "            # Extract href attributes\n",
    "            hrefs = [link.get_attribute(\"href\") for link in links]\n",
    "            \n",
    "            # Write hrefs to a file, using a unique file for each URL\n",
    "            for href in hrefs:\n",
    "                file.write(href + '\\n')\n",
    "        \n",
    "            print(f\"Extracted {len(hrefs)} links from {url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from {url}: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicate links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_links(file_path):\n",
    "    try:\n",
    "        # Read all lines (links) from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            links = file.readlines()\n",
    "\n",
    "        # Remove any leading/trailing whitespace from each link and remove duplicates\n",
    "        unique_links = list(set(link.strip() for link in links))\n",
    "\n",
    "        # Sort the links (optional, if you want to keep them in order)\n",
    "        unique_links.sort()\n",
    "\n",
    "        # Write the unique links back to the file\n",
    "        with open(file_path, 'w') as file:\n",
    "            for link in unique_links:\n",
    "                file.write(link + '\\n')\n",
    "\n",
    "        print(f\"Duplicates removed. {len(links) - len(unique_links)} duplicate(s) found.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'links.txt' with the path to your .txt file\n",
    "file_path = 'scrapedsublink.txt'\n",
    "remove_duplicate_links(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Docs link in TXT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up Chrome options (headless or visible browsing)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode if you don't need browser UI\n",
    "\n",
    "# Path to the Chrome WebDriver executable\n",
    "chrome_driver_path = '/home/aiobc/Desktop/projects/testing/datascraping/chromedriver-linux64/chromedriver'\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Read URLs from a text file\n",
    "with open('from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up Chrome options (headless or visible browsing)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode if you don't need browser UI\n",
    "\n",
    "# Path to the Chrome WebDriver executable\n",
    "chrome_driver_path = '/home/aiobc/Desktop/projects/testing/datascraping/chromedriver-linux64/chromedriver'\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Read URLs from a text file\n",
    "with open('urls.txt', 'r') as file:\n",
    "    urls = [line.strip() for line in file if line.strip()]  # Reading and stripping each line\n",
    "\n",
    "# Function to scrape DOC links from a given URL\n",
    "def scrape_doc_links(url):\n",
    "    print(f\"Scraping {url}...\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait until the table is present (maximum 10 seconds wait)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'versionContent')))\n",
    "        print(\"Table found!\")\n",
    "\n",
    "        # Find the table rows\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, 'table#versionContent tbody tr')\n",
    "        print(f\"Total rows found: {len(rows)}\")  # Debug print to check if rows are found\n",
    "\n",
    "        # Iterate through the rows\n",
    "        for row in rows:\n",
    "            # Check if the row contains the green circle span\n",
    "            green_circle = row.find_elements(By.CSS_SELECTOR, 'span.circle.soft-green')\n",
    "\n",
    "            if green_circle:\n",
    "                print(\"Green circle found in row!\")  # Debug print to confirm green circle found\n",
    "                # If the green circle is found, look for the DOC link in the same row\n",
    "                doc_link = row.find_element(By.XPATH, './/a[contains(@href, \".docx\")]')\n",
    "\n",
    "                if doc_link:\n",
    "                    # Print the DOC link URL\n",
    "                    with open('doc_links.txt', 'a') as file:\n",
    "                        file.write(f'{doc_link.get_attribute(\"href\")}\\n')\n",
    "                    break  # Exit loop after the first match\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping {url}: {e}\")\n",
    "\n",
    "# Loop through the list of URLs and scrape each one\n",
    "for url in urls:\n",
    "    scrape_doc_links(url)\n",
    "\n",
    "# Close the browser after scraping all URLs\n",
    "driver.quit()\n",
    ".txt', 'r') as file:\n",
    "    urls = [line.strip() for line in file if line.strip()]  # Reading and stripping each line\n",
    "\n",
    "# Function to scrape DOC links from a given URL\n",
    "def scrape_doc_links(url):\n",
    "    print(f\"Scraping {url}...\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait until the table is present (maximum 10 seconds wait)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'versionContent')))\n",
    "        print(\"Table found!\")\n",
    "\n",
    "        # Find the table rows\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, 'table#versionContent tbody tr')\n",
    "        print(f\"Total rows found: {len(rows)}\")  # Debug print to check if rows are found\n",
    "\n",
    "        # Iterate through the rows\n",
    "        for row in rows:\n",
    "            # Check if the row contains the green circle span\n",
    "            green_circle = row.find_elements(By.CSS_SELECTOR, 'span.circle.soft-green')\n",
    "\n",
    "            if green_circle:\n",
    "                print(\"Green circle found in row!\")  # Debug print to confirm green circle found\n",
    "                # If the green circle is found, look for the DOC link in the same row\n",
    "                doc_link = row.find_element(By.XPATH, './/a[contains(@href, \".docx\")]')\n",
    "\n",
    "                if doc_link:\n",
    "                    # Print the DOC link URL\n",
    "                    with open('doc_links.txt', 'a') as file:\n",
    "                        file.write(f'{doc_link.get_attribute(\"href\")}\\n')\n",
    "                    break  # Exit loop after the first match\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping {url}: {e}\")\n",
    "\n",
    "# Loop through the list of URLs and scrape each one\n",
    "for url in urls:\n",
    "    scrape_doc_links(url)\n",
    "\n",
    "# Close the browser after scraping all URLs\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Path to your chromedriver\n",
    "CHROME_DRIVER_PATH = '/home/aiobc/Desktop/projects/testing/datascraping/chromedriver-linux64/chromedriver'\n",
    "\n",
    "# Path to the .txt file containing the URLs\n",
    "LINKS_FILE = 'urls.txt'\n",
    "\n",
    "# Function to read links from .txt file\n",
    "def read_links(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        links = [line.strip() for line in file.readlines()]\n",
    "    return links\n",
    "\n",
    "# Function to open links in Chrome\n",
    "def open_links_in_chrome(links):\n",
    "    # Setup Chrome options\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Initialize the Chrome driver\n",
    "    service = Service(CHROME_DRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # Loop through the links and open each in Chrome\n",
    "    for link in links:\n",
    "        driver.get(link)\n",
    "        time.sleep(5)  # Adjust the sleep time as needed\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    links = read_links(LINKS_FILE)\n",
    "    open_links_in_chrome(links)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
